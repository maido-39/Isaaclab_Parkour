seed: 1
device: cuda:0
num_steps_per_env: 24
max_iterations: 50000
empirical_normalization: false
policy:
  class_name: ActorCriticRMA
  init_noise_std: 1.0
  noise_std_type: scalar
  actor_hidden_dims:
  - 512
  - 256
  - 128
  critic_hidden_dims:
  - 512
  - 256
  - 128
  activation: elu
  tanh_encoder_output: false
  scan_encoder_dims:
  - 128
  - 64
  - 32
  priv_encoder_dims:
  - 64
  - 20
  actor:
    num_priv_explicit: 9
    num_priv_latent: 29
    num_prop: 53
    num_scan: 132
    num_hist: 10
    class_name: Actor
    state_history_encoder:
      num_priv_explicit: 9
      num_priv_latent: 29
      num_prop: 53
      num_scan: 132
      num_hist: 10
      class_name: StateHistoryEncoder
      channel_size: 10
algorithm:
  class_name: PPOWithExtractor
  value_loss_coef: 1.0
  use_clipped_value_loss: true
  clip_param: 0.2
  entropy_coef: 0.01
  num_learning_epochs: 5
  num_mini_batches: 4
  learning_rate: 0.0002
  schedule: adaptive
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.01
  max_grad_norm: 1.0
  normalize_advantage_per_mini_batch: false
  symmetry_cfg: null
  rnd_cfg: null
  dagger_update_freq: 20
  priv_reg_coef_schedual:
  - 0.0
  - 0.1
  - 2000.0
  - 3000.0
clip_actions: null
save_interval: 100
experiment_name: unitree_go2_parkour
run_name: ''
logger: tensorboard
neptune_project: isaaclab
wandb_project: isaaclab
resume: false
load_run: .*
load_checkpoint: model_.*.pt
estimator:
  num_priv_explicit: 9
  num_priv_latent: 29
  num_prop: 53
  num_scan: 132
  num_hist: 10
  class_name: DefaultEstimator
  train_with_estimated_states: true
  learning_rate: 0.0001
  hidden_dims:
  - 128
  - 64
depth_encoder: null
